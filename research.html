<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Sonal Kumar | Research</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Fonts and Icons -->
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <!-- Custom Stylesheet -->
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <!-- Navigation Bar -->
    <header>
        <nav>
            <div class="logo"><a href="index.html">Sonal Kumar</a></div>
            <input type="checkbox" id="nav-toggle">
            <label for="nav-toggle" class="icon-burger">
                <div class="line"></div>
                <div class="line"></div>
                <div class="line"></div>
            </label>
            <ul class="nav-menu">
                <li><a href="index.html" class="nav-link">About Me</a></li>
                <li><a href="research.html" class="nav-link active">Research</a></li>
                <li><a href="awards.html" class="nav-link">Awards</a></li>
                <li><a href="news.html" class="nav-link">News</a></li>
            </ul>
        </nav>
    </header>

    <!-- Research Section -->
    <section id="research">
        <div class="section-container">
            <h2>Research</h2>

            <!-- Project Section -->
            <section class="project-section">
                <div class="container">
                    <div class="text-content">
                        <h2>GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities</h2>
                        <p>We propose GAMA, a novel Large Audio-Language Model (LALM) that is capable of responding accurately to complex questions about an input audio. GAMA benefits from a mixture of encoders and synthetic data generated using a novel data generation pipeline we propose. GAMA currently stands as the state-of-the-art LALM on various audio understanding, reasoning, and hallucination benchmarks.
                        </p>
                        <div class="buttons">
                            <a href="https://arxiv.org/abs/2406.11768" target="_blank" class="btn">arXiv</a>
                            <a href="https://sreyan88.github.io/gamaaudio/" target="_blank" class="btn">Homepage</a>
                            <a href="https://github.com/Sreyan88/GAMA" target="_blank" class="btn">Code</a>
                            <a href="https://huggingface.co/spaces/sonalkum/GAMA" target="_blank" class="btn">GAMA Demo</a>
                            <a href="https://huggingface.co/spaces/sonalkum/GAMA-IT" target="_blank" class="btn">GAMA-IT Demo</a>
                        </div>
                    </div>
                    <div class="image-content">
                        <img src="images/gama.jpg" alt="Synthio Project Image">
                    </div>
                </div>
            </section>
            <section class="project-section">
                <div class="container">
                    <div class="text-content">
                        <h2>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</h2>
                        <p>We introduce MMAU (Massive Multi-Task Audio Understanding and Reasoning Benchmark), a comprehensive benchmark 
                            designed to evaluate Large Audio-Language Models (LALMs) on tasks that demand expert-level knowledge and complex 
                            reasoning. MMAU includes 10,000 meticulously curated audio clips paired with human-annotated natural language questions 
                            and answers, covering speech, environmental sounds, and music. The benchmark features information extraction and 
                            reasoning questions that require models to demonstrate 27 distinct skills across unique and challenging tasks. Notably,
                             even the advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves 52.50%,
                              underscoring significant potential for improvement.</p>
                              <div class="buttons">
                                <a href="https://arxiv.org/pdf/2410.19168" target="_blank" class="btn">arXiv</a>
                                <a href="https://sakshi113.github.io/mmau_homepage/" target="_blank" class="btn">Homepage</a>
                                <a href="https://github.com/Sakshi113/mmau/tree/main" target="_blank" class="btn">Code</a>
                                <a href="https://eval.ai/web/challenges/challenge-page/2391/overview" target="_blank" class="btn">EvalAI</a>
                            </div>
                    </div>
                    <div class="image-content">
                        <img src="images/mmau-hero.jpg" alt="Synthio Project Image">
                    </div>
                </div>
            </section>
            <section class="project-section">
                <div class="container">
                    <div class="text-content">
                        <h2>Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data</h2>
                        <p>We present Synthio, a novel method for generating synthetic data specifically for audio classification. 
                            Our approach first involves aligning a Text-to-Audio generation model with the target dataset through 
                            preference optimization. We then introduce an iterative prompting method with large language models (LLMs) 
                            to generate diverse and consistent audio captions, which are used to prompt the Text-to-Audio generation model 
                            for synthetic data creation. By augmenting small-scale audio classification datasets with data generated by Synthio,
                            we achieve up to a 39% performance improvement on benchmark datasets.</p>
                        <div class="buttons">
                            <a href="https://arxiv.org/pdf/2410.02056" target="_blank" class="btn">arXiv</a>
                            <a href="https://github.com/Sreyan88/Synthio" target="_blank" class="btn">Code</a>
                            <a href="https://huggingface.co/spaces/sonalkum/synthio-stable-audio-open" target="_blank" class="btn">Demo</a>
                        </div>
                    </div>
                    <div class="image-content">
                        <img src="images/synthio.jpg" alt="Synthio Project Image">
                    </div>
                </div>
            </section>
            <section class="project-section">
                <div class="container">
                    <div class="text-content">
                        <h2>CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models</h2>
                        <p>We introduce CompA, a benchmark specifically designed to address gaps in compositional reasoning in 
                            audio-language models (ALMs). CompA includes two expert-annotated benchmarks: CompA-order, which evaluates 
                            how well an ALM understands the sequence of acoustic events, and CompA-attribute, which tests the modelâ€™s ability 
                            to associate attributes with specific sounds. Each test instance contains audio-caption pairs with the same events 
                            but in varying compositions, challenging the model to match audio accurately to captions. Using CompA, we demonstrate 
                            that current ALMs, including CLAP, struggle with complex compositional reasoning. To improve performance, we propose 
                            CompA-CLAP, a fine-tuned model that leverages compositionally-aware hard negatives and a new modular contrastive learning 
                            objective, significantly enhancing compositional reasoning capabilities across both benchmarks</p>
                            <div class="buttons">
                                <a href="https://arxiv.org/pdf/2310.08753" target="_blank" class="btn">arXiv</a>
                                <a href="https://sreyan88.github.io/compa_iclr/" target="_blank" class="btn">Homepage</a>
                                <a href="https://github.com/Sreyan88/CompA" target="_blank" class="btn">Code</a>
                            </div>
                    </div>
                    <div class="image-content">
                        <img src="images/compa.png" alt="Synthio Project Image">
                    </div>
                </div>
            </section>
            <section class="project-section">
                <div class="container">
                    <div class="text-content">
                        <h2>EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning</h2>
                        <p>We introduce EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised approach for speech representation learning.
                            EH-MAM enables better learning from unsupervised data by using an adaptive masking strategy that gradually increases the difficulty of the p
                            re-text SSL task and selectively reconstructing challenging regions within the speech input. EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks
                            by 5%-10%.</p>
                            <div class="buttons">
                                <a href="https://arxiv.org/pdf/2410.13179" target="_blank" class="btn">arXiv</a>
                                <a href="https://github.com/cs20s030/ehmam" target="_blank" class="btn">Code</a>
                                <a href="https://drive.google.com/file/d/1Rx4MpeN1-0xjjKXx5zbJMCCvGLdVe1nr/view?usp=sharing" target="_blank" class="btn">Checkpoint</a>
                            </div>
                    </div>
                    <div class="image-content">
                        <img src="images/eh-mam.jpg" alt="Synthio Project Image">
                    </div>
                </div>
            </section>
            <!-- End of Project Section -->

            <!-- You can add more project sections here -->

        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="footer-container">
            <div class="social-icons">
                <a href="https://www.linkedin.com/in/realsonalkumar/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                <a href="https://github.com/your-profile" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
                <a href="https://twitter.com/your-profile" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
                <a href="https://scholar.google.com/citations?user=your-id" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
                </a>
            </div>
            <p>&copy; 2023 Sonal Kumar. All Rights Reserved.</p>
        </div>
    </footer>

    <!-- JavaScript -->
    <script src="script.js"></script>
</body>
</html>